{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBxm68xmgill",
        "outputId": "0b2b66d9-19fc-4de5-81b6-8992d3b36ea0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'chatbot_qa'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 11 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (11/11), 745.27 KiB | 5.65 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/EnriqueMejia96/chatbot_qa.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9nlcp-kmBcC",
        "outputId": "0ffabd21-b2ef-4e7b-9b38-87478bd6b2fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/chatbot_qa\n"
          ]
        }
      ],
      "source": [
        "%cd chatbot_qa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNh5ScGhmE3K",
        "outputId": "14ad51e0-218b-4b60-e867-e79a1f74ad01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 956\n",
            "drwxr-xr-x 4 root root   4096 Jan 11 22:14 .\n",
            "drwxr-xr-x 1 root root   4096 Jan 11 22:03 ..\n",
            "-rw-r--r-- 1 root root   2832 Jan 11 21:51 app.py\n",
            "-rw-r--r-- 1 root root     76 Jan 11 22:03 credentials.json\n",
            "-rw-r--r-- 1 root root   3921 Jan 11 21:51 dmc_logo.jpg\n",
            "drwxr-xr-x 8 root root   4096 Jan 11 21:51 .git\n",
            "-rw-r--r-- 1 root root  50192 Jan 11 21:51 Lab_LLM_intro.ipynb\n",
            "-rw-r--r-- 1 root root 885970 Jan 11 21:51 llm_doc.pdf\n",
            "drwxr-xr-x 2 root root   4096 Jan 11 22:14 __pycache__\n",
            "-rw-r--r-- 1 root root     12 Jan 11 21:51 README.md\n",
            "-rw-r--r-- 1 root root   1822 Jan 11 21:51 utils.py\n"
          ]
        }
      ],
      "source": [
        "!ls -la"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwvEI5c5fdd6"
      },
      "source": [
        "# 0. Instalación de librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56NzsFMWE8dr"
      },
      "outputs": [],
      "source": [
        "# !pip install streamlit\n",
        "# !pip install pyngrok==4.1.1\n",
        "# #https://dashboard.ngrok.com/signup\n",
        "# !pip install --upgrade typing_extensions\n",
        "# !pip install openai\n",
        "# !pip install pypdf\n",
        "# !pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLjT4hG8fj8v"
      },
      "source": [
        "# 1. Crear Vector Store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE39ibAXf5DP"
      },
      "source": [
        "## 1.1. Carga de documento pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xwsglPNX0FuP"
      },
      "outputs": [],
      "source": [
        "# Importe la clase PyPDFLoader del módulo langchain.document_loaders.\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# Cree una instancia de la clase Py\n",
        "# PDFLoader, pasando el nombre del archivo del documento PDF que desea cargar.\n",
        "loader = PyPDFLoader(\"llm_doc.pdf\")\n",
        "\n",
        "# Este método lee el contenido del archivo PDF especificado al crear la instancia del cargador.\n",
        "# El contenido del documento cargado se almacena en la variable 'documentos' para su posterior procesamiento.\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='Building Pipelines and Environments for  \\nLarge Language Models\\nWhitepaper by Dr. Archisman Majumdar, Principal – Mphasis NEXT Labs | Amrit R, Lead Data Scientist - Mphasis NEXT Labs |  \\nManami Mandal, Lead Data Scientist - Mphasis NEXT Labs | Dr. Udayaadithya Avadhanam, Principal & VP , Mphasis NEXT Labs |  \\nSai Bharath Sundar, Manager (Data Science), Mphasis  NEXT Labs | Biju Mathews, Partner – Mphasis NEXT Labs  |\\nSiddharth Shankar, Affiliated to University of Maryland', metadata={'source': 'llm_doc.pdf', 'page': 0}),\n",
              " Document(page_content='Contents\\nIntroduction to LLMOps 1\\nWhy LLMOps? 1\\nTypical stages in an LLMOps Workflow 2\\n 3.1 Data Collection, Preparation, Labelling 2\\n 3.2 Selection of Foundation Models 3\\n 3.3 Using Large Language Models - Prompting and Fine-tuning 4\\n 3.4 Evaluation of Prompts and Models & Version Control 5\\n 3.5 Deployment and Monitoring 6\\n 3.6 Security, Privacy, Governance and Ethical Considerations 8\\nSetting Up LLMOps Pipelines 9\\nConclusions 11\\nReferences 11', metadata={'source': 'llm_doc.pdf', 'page': 1}),\n",
              " Document(page_content='1.\\nIntroduction to LLMOps\\nGenerative AI models have gained wide popularity in recent times with the adoption of  \\ntransformer-based neural network architectures. Generative model’s ability to generate new data \\nenables them to go beyond traditional prediction and classification use cases. These models \\nare now used across domains and use cases like chatbots, question answering, fraud detection, \\nprotein folding and many more.\\nGenerative AI models for natural language use cases are powered by Large Language Models (LLMs). \\nLLMs are transformer-based Deep Learning architectures that harness vast amounts of textual \\ndata to develop language and domain understanding. The models are built with an emphasis on \\ngenerating human-like responses and reasoning. Their ability to understand human languages allows \\nthem to serve as powerful tools for information retrieval, natural language processing, language \\ntranslation and even creative writing.\\nUsing large language models in production environments poses a certain unique set of challenges \\nsuch as organizing LLMs into agents for sub-tasks, developing robust instructions for each  \\nLLM agent, evaluating the correctness of generated response and efficiencies with fine-tuning.  \\nHence, effective usage in a production environment requires appropriate infrastructure and practices \\nfocusing on experimentation, deployment, management and monitoring of large language models. \\nLarge Language Model Operations (LLMOps) is a framework of tools and best practices  \\nto manage the lifecycle of LLM-powered applications, from development to deployment  \\nand maintenance.\\nThe aim is to enable AI capabilities with LLMs by developing better prompts, longer context, \\nfaster inference and customized techniques that enable rapid experimentation and innovation \\nwith LLMs. Together, these allow data scientists and engineers to collaborate effectively and \\ndeliver high-quality solutions.\\nIn this paper, we highlight some of the recent advances, challenges, common frameworks, \\nplatforms and tools that are used in a typical LLMOps workflow. We conclude by demonstrating the \\ncomponents of a typical LLMOps environment for production.\\n2.\\nWhy LLMOps?  \\nLLMOps aims to focus on overcoming the unique challenges and requirements specific to LLM \\nmodels such as:\\n• Infrastructure, scaling and performance:  LLMs require significant computational resources \\nfor text data collection. LLMs are often composed of multiple layers and components, such as \\ntransformers, recurrent neural networks, attention mechanisms and embeddings. Training these \\nmodels requires high computational resources and efficient optimization algorithms. \\n|  1', metadata={'source': 'llm_doc.pdf', 'page': 2}),\n",
              " Document(page_content='2  |2  |• The ambiguity of natural languages:  Prompts can be interpreted differently by users and LLMs, \\nleading to silent failures or unexpected outputs. Prompt engineering requires rigorous evaluation, \\nversioning and optimization methods to ensure quality and consistency.\\n• Backward and forward compatibility:  LLMs are constantly evolving and being updated, which \\ncan affect the behavior and performance of existing prompts. Prompt engineering needs to \\naccount for the compatibility issues between different versions of LLMs and ensure smooth \\ntransitions. Steps such as model fine-tuning, deployment and retraining could affect the \\nbackward-forward compatibility of the model.\\n• Cost and latency:  LLMs are expensive to run and can have high latency for real-time \\napplications. Cost and latency analysis for LLMs is difficult due to the variability of inputs and \\noutputs. LLMs also need to be accelerated with techniques such as inference with reference, \\nprompt tuning or distillation. \\n• Data privacy and compliance:  Data privacy and compliance are critical issues for large \\nlanguage models like ChatGPT. These models are trained on massive amounts of text data from \\nvarious sources, which may contain sensitive or personal information.\\n3.\\nTypical Stages in an LLMOps Workflow \\nLLMOps comprise various components/stages involved in the entire lifecycle of a large language \\nmodel from data ingestion to model deployment and monitoring. Some of these include sourcing \\nthe data, cleaning, choosing the right architecture, model training, fine-tuning, monitoring, \\nmaintenance and optimization to list some. In this section, we highlight the typical activities in \\neach stage.\\n3.1 Data Collection, Preparation, Labelling\\nThe data preparation stage for Machine Learning involves several steps: sourcing data, ensuring \\ncompleteness, adding labels and data transformations to generate features. LLMs require massive \\namounts of data to train. This can be a challenge to collect, especially if the data is specific to a \\nparticular domain or task.\\nThe quality of the data is critical for the performance of large language models. Any bias or \\ninconsistencies in the data can be amplified by the model leading to poor performance. If the \\ndata is noisy or inconsistent, it can lead to errors in the model, such as generating incorrect or \\nnonsensical text. Labelling data for such tasks can be an expensive, time-consuming and complex \\nactivity. This requires human experts, domain knowledge or specific guidelines. Without Large \\nLanguage, models can frequently produce results that are unsuited for real-world applications. \\nThus, reinforcement learning with human feedback has emerged as an area of great importance in \\nthe training of large language models.\\nOnly a few companies train benchmark models from scratch since they need large computing \\nbudgets, hardware resources and extensive ML knowledge.  \\nA complete discussion on the training of large language models is beyond the scope of this paper.', metadata={'source': 'llm_doc.pdf', 'page': 3}),\n",
              " Document(page_content='|  33.2 Selection of Foundation Models \\nSince training LLMs from scratch is costly and time-consuming, requiring enormous amounts of \\ndata and computational resources, many researchers and practitioners opt to use pre-trained LLMs \\nas foundation models, which can be fine-tuned or adapted to specific downstream applications \\nwith less data and resources. \\nSome popular foundation models include BERT, GPT-3, T5 and XLNet. These models have been \\nshown to be effective for a wide range of tasks including natural language understanding, natural \\nlanguage generation and machine translation. \\nDifferent foundation models have different strengths and weaknesses depending on their \\narchitecture, pre-training data and learning objectives. It is important to carefully consider the \\ndifferent factors when selecting a foundation model for an LLM project, some points that can be \\nconsidered are:\\n• Performance of model on benchmark tasks\\n Several public benchmarks offer comprehensive evaluations of large language models across  \\n various tasks and datasets. Some examples of these are HELM, GLUE, SuperGLUE,  \\n Big-Bench and MMLU.\\n• Size and intricacy of the model\\n However, many of the models require expensive and high-performance hardware  \\n to run and deploy. Models like LLaMA and ALPACA are built on principles of  \\n community-driven innovation and wide accessibility, contributing to the democratization of   \\n AI technology. Increasingly, open-source LLaMA-inspired models, such as Vicuna,  \\n a tuned version of LLaMA that matches GPT-4 performance, Koala from Berkeley AI Research  \\n Institute and ColossalChat from UC Berkeley, which is a ChatGPT-like model, are becoming   \\n available to run on less expensive hardware. \\n• Availability of domain-specific pre-trained models\\n Domain-specific models are foundational models that are specialized for tasks that can be   \\n selected as per use cases. Some examples are BloombergGPT which is trained on  a wide  \\n range of financial data for the financial industry, Med-PaLM2 and BioMedLM which are   \\n aligned to the medical domain to answer medical questions more accurately and safely.   \\n Galactica which is an LLM fine-tuned with scientific knowledge.\\n• Licensing terms of the foundation model\\n One of the key factors that influence the choice of an LLM is the licensing and availability  \\n of the model. Many organizations can opt for a managed model service provided by various   \\n vendors where for a flat fee they get a fully managed deployment service and technical  \\n support with only the option to fine-tune. Another option is to use an open-source model  \\n and manage the deployment. This enables the flexibility of fine-tuning and retraining.\\n• Ethical and social implications\\n We discuss the ethical and social implications of model selection in section 3.6 Security,   \\n Privacy, Governance and Ethical Considerations.', metadata={'source': 'llm_doc.pdf', 'page': 4}),\n",
              " Document(page_content='3.3 Using Large Language Models - Prompting and Fine-tuning\\nLarge Language models have been developed as powerful models for obtaining labels on text  \\ndata which can complete a variety of tasks without any additional training or examples, i.e., with \\nZero-Shot Learning (ZSL). The goal of zero-shot learning is to develop the capacity to forecast \\noutcomes without the use of training samples; to do this, the machine must be able to also identify \\nobjects belonging to classes that were not previously taught.\\nThe foundation of zero-shot learning is the transmission of knowledge that is already present in \\nthe examples provided during training. Contextual embeddings in LLMs can identify the semantic \\nconcepts within the text making it helpful for ZSL. The key to this is robust pre-training using varied \\nprompt patterns and techniques. \\nPrompting refers to the bundling of queries intended to gain information from the model in the form \\nof natural language processing. Prompt templates come as a useful way to generate and handle \\nprompts. A prompt template usually consists of:\\n• Instructions for the language model\\n• A set of a few shot examples to help the language model generate a better response  \\n (an optional requirement in the case of ZSL)\\n• And a question for the language model\\nThe quality of the prompt used as input for training LLMs is important as LLMs can practice  \\nzero-shot or few-shot learning just with contextual prompts without optimizing parameters.  \\nA collection of such templates and prompts can be stored in the form of a prompt library.  \\nThese libraries include prompts and templates of various categories such as editing, SEO, \\ncontent creation, etc.\\nFor more complicated tasks, where zero-shot prompting may not suffice, LLMs can be given a \\nfew examples to enable in-context learning. In the few-shot learning scenario, the LLM sees a few \\nexamples of input-output pairs and solves new tasks.\\nFew-shot or zero-shot prompting is not always sufficient in customizing model behavior \\nfor complex tasks. In such scenarios, tuning of the models may be required to improve the \\nperformance of the model to deliver specific and optimal outputs. There are two primary ways in \\nwhich a model can be tuned: Fine-tuning and Parameter-efficient Fine-tuning.\\n• Fine-tuning:  In this method, the model is pre-trained on a generic dataset and after that, it   \\nis copied and retrained using the learned weights. The challenge of fine-tuning an LLM model  \\nis that since it is a large model, it takes an exceptionally long time to update every weight.   \\nTherefore, parameter-efficient tuning is preferred more in the case of LLMs.\\n• Parameter Efficient Fine-Tuning (PEFT) : It includes training only a small subset of    \\n parameters that are identified as the most important ones out of the lot. These parameters   \\n could either be new or existent. With PEFT, most parameters of the pre-trained model    \\n are unchanged which makes it less likely to fall into Catastrophic Forgetting compared to   \\n full fine-tuning which revises the weights of all the parameters making the model more prone   \\n to Catastrophic Forgetting (a phenomenon in which the model loses its generalization ability   \\n on a task after being trained on a new task because of overriding of the weights that have   \\n been learned in the past, thus affecting model performance for the tasks learned previously).   \\n2  |4  |', metadata={'source': 'llm_doc.pdf', 'page': 5}),\n",
              " Document(page_content=\" Two of the most common ways of applying PEFT are using adapters, which are submodules   \\n inserted in the specific layers in the model architecture and via Low-Rank Adaption (LoRA) which  \\n is also like adapters in functionality. \\nInstruction Fine-tuning or Instruction Tuning is also a well-known method of tuning pre-trained \\nmodels. Fine-tuned Language Net (FLAN) is a pre-trained model that is tuned based on \\ninstructions. FLAN’s instruction tuning phase requires less updates than the model’s pre-training \\nphase, which requires a significant amount of computation. Creating a dataset of instructions from \\nscratch to fine-tune the model would take many resources. Hence, existing templates are used \\nto convert the dataset into instruction format. Research shows that model scale is important for \\nthe model’s ability to benefit from instruction tuning. The FLAN approach reduces performance at \\nsmaller sizes, and it is only at higher scales that the model can generalize from the training data's \\ninstructions to new tasks. This may be because too small models lack the parameters necessary to \\ncarry out a wide range of tasks.\\n3.4 Evaluation of Prompts and Models and Version Control\\nEvaluating the output of large language models can be a challenge because they often  \\nproduce outputs that are not directly comparable to the ground truth or human expectations. \\nWhile evaluating a typical discriminative model, the performance can be measured by metrics \\nsuch as accuracy, precision, recall or F1-score because they usually produce outputs that are \\ndiscrete, categorical or numerical. LLM evaluation, however, is multi-dimensional, hence a \\ncomprehensive evaluation framework of the models and data (prompts) is necessary to achieve \\noptimal results. Performance evaluation of working models of LLM becomes necessary as it \\nis tested on metrics to conclude how well the model is performing on a varied range of inputs \\nsuch as accuracy, coherence, fluency and subject relevance. It is frequently required to combine \\ndifferent methodologies to fully assess a language model's performance. Following are the five \\ncommonly used evaluation methods for LLMs :\\n• Perplexity:  It gives results in figurative/quantitative format to determine the level  \\n of performance of the model. The lower the perplexity, the better the performance.  \\n It calculates how much the model is surprised by seeing new data. Perplexity is usually used   \\n just to determine how extensively a model has learned the training set. Other metrics like   \\n BLEU and ROUGE are used on the test set to measure performance.\\n• BLEU (Bilingual Evaluation Understudy): This metric is commonly used in machine    \\n translation tasks. The generated output is compared with one or more reference translations   \\n and then the similarity between them is calculated. The range of scores for BLEU is from  \\n 0 to 1 with scores nearer to 1 suggesting better performance.\\n• ROUGE (Recall-Oriented Understudy for Gisting Evaluation):  It is used to determine   \\n the quality of summaries. It compares the generated summary with one or more reference   \\n summaries. It calculates precision, recall and F-1 score. ROUGE scores around fifty are   \\n considered excellent results. ROUGE is available in three different forms: rouge-n, which is   \\n the most common and finds n-gram overlap; rouge-l, which locates the Longest Common   \\n Subsequence; and rouge-s, which focuses on skip grams.\\n|  5\", metadata={'source': 'llm_doc.pdf', 'page': 6}),\n",
              " Document(page_content='• METEOR Score:  It measures the quality of generated text based on the alignment between   \\n the generated text and the reference text. It is a harmonic mean of unigram precision and   \\n recall, with recall weighted higher than precision. This metric produces a good correlation with  \\n human judgement at the sentence or segment level.\\n• BERTScore:  The pre-trained contextual embeddings from BERT are used by BERTScore   \\n to compare words in candidate and reference sentences based on their cosine similarity.  \\n It has been demonstrated that it correlates with human judgment in the evaluation of    \\n sentences and systems. BERTScore computes precision, recall and F1 measures, which can   \\n be useful for evaluating different language generation tasks as well.\\nEvaluation metrics for multimodal LLMs:  In recent experiments, the Multimodal Large Language \\nModel (MLLM), which is built on the potent LLM, has shown extraordinary emergent skills including \\nthe ability to create poetry based on images. The various benchmarks for evaluation of such \\nmodels are under ongoing research but here are a few prominent metrics more widely used:\\n• Caption Hallucination Assessment with Image Relevance (CHAIR):  It is a well-liked   \\n statistic for assessing object hallucination in activities requiring picture captioning.  \\n CHAIR determines the percentage of things that are in the image, but not the caption based   \\n on the actual objects in the image. The two variations, CHAIRi (instance-level) and CHAIRs   \\n (sentence-level), which assess the degree of hallucination at the object instance level and   \\n phrase level, respectively, are often used in existing work.\\n• Retrieval evaluation:  Consider the task of retrieving images from sentence queries. Given a   \\n test sentence, we compute the model perplexity conditioned on each test image and rank   \\n each image accordingly.\\n• Benchmark evaluation:  It is another frequently used technique using which functionality and  \\n performance of models are evaluated. LLM benchmarking can be conducted with respect   \\n to various tasks. It is used to perform task-specific evaluation or assess the model’s output   \\n for fairness, coherence, relevance which is known as bias and fairness evaluation. The AI2   \\n Reasoning Challenge (ARC) and WinoGrande, for instance, are two well-liked benchmarks for  \\n evaluating LLMs like OpenAI’s GPT-4, Databricks’ Dolly and Facebook’s LLaMA. Another   \\n important evaluation benchmark is Google’s BIG (Beyond the Imitation Game) Bench. One of the  \\n project’s goals is to see how the performance on the tasks is changing with the model size, with  \\n the size ranging by many orders of magnitude. BIG bench currently consists of 204 tasks.\\n3.5 Deployment and Monitoring\\nThe optimal deployment strategy for each large language model can vary according to the use \\ncases. Following are some of the key features of the LLM deployment:\\n• Hassle-free deployment: Models can be deployed as production-ready APIs easily with a   \\n few steps rather than handling MLOps infrastructure.\\n• Cost efficiency: Automatic scaling benefits through scaling down the infrastructure when the  \\n endpoints are not in use, reducing the overall cost.\\n• Optimization:  Utilizing proprietary transformer code, Flash Attention Power from Text    \\n Generation Inference and Paged Attention to enable high throughput with minimal latency.\\n2  |6  |', metadata={'source': 'llm_doc.pdf', 'page': 7}),\n",
              " Document(page_content='• Security: For improved data security and compliance, deploy models at secure offline   \\n endpoints that can only be accessed through direct VPC (Virtual Private Cloud) connections   \\n are SOC2 Type 2 certified, and have BAA and GDPR (General Data Protection Regulation)   \\n data processing agreements.\\nOne way to facilitate the deployment of these models is to apply some optimization techniques \\nafter training. These techniques reduce the model size, which lowers the infrastructure costs \\nand increases the inference speed. Some examples of these techniques are model distillation, \\nquantization and pruning.\\n• Distillation: This process involves training a separate inference-optimized model using  \\n the training-optimized model, where knowledge transfer happens between the two.   \\n Model distillation allows the inference-optimized model to keep almost all the quality of   \\n the training-optimized model. Stanford Alpaca: An instruction-following LLaMA Model,   \\n this paper shows the fine-tuning of a smaller open-source language model (LLaMA-7B, the   \\n seven billion parameter version of LLaMA) on examples generated by a larger language model  \\n (text-davinci-003 – 175 billion parameters). For fine-tuning, they used 52K instructions, which  \\n they input into text-davinci-003 to obtain outputs, which are then used to fine-tune LLaMA-7B.  \\n This costs under $500 to generate. The training process for fine-tuning costs under $100.\\n• Quantization:  This is a common compression operation that is used to reduce the memory   \\n footprint of the model and helps improve the inference performance.\\n• Pruning:  This technique is used to reduce the size and computational complexity of an LLM   \\n by removing redundant or unnecessary model parameters. Pruning improves the efficiency of  \\n LLM inference without sacrificing accuracy. Pruning can operate at two different levels: weight  \\n pruning and neuron pruning. Weight pruning eliminates individual connections between   \\n neurons, while neuron pruning removes entire neurons or units from the network.\\nDistributed inference: Models served in production environments are subject to multiple parallel \\nrequests for inference. Distributed hardware can be optimally leveraged for inference. In a multi-\\nGPU setup, this involves identifying the number of available GPUs and processing incoming \\nrequests parallelly, i.e., assigning a request to a GPU. This can be either real-time or batch (where \\nbatches are split and assigned) requests.\\nThe distributed inference is not only applicable to multi-request parallelization, but also to \\ncases where a single request requires to be split into multiple parallel tasks where each task is \\naccomplished by an LLM in a distributed setup. \\nModel monitoring helps maintain the integrity of the model’s performance. E.g., in Vertex AI,  \\nthe model is monitored post-deployment based on prediction input data for feature skew and drift. \\nTensorFlow Data Validation (TFDV) is used in model monitoring to compute the distributions and \\ndistance scores for detecting training-serving skew and prediction drift. As LLMs are used in more \\napplications, it is important to keep an eye on their behavior and put safety measures in place to \\navoid possible problems like poisonous prompts and replies or the existence of sensitive material. \\nTo make sure their behavior adheres to the specified norms and rules, large language models  \\nlike GPT-3 are routinely monitored after deployment using a combination of automated and  \\nhuman techniques.\\n|  7', metadata={'source': 'llm_doc.pdf', 'page': 8}),\n",
              " Document(page_content=\"Model monitoring helps maintain the integrity of the model’s performance. E.g., in Vertex AI,  \\nthe model is monitored post-deployment based on prediction input data for feature skew and \\ndrift. Feature skew occurs when there is a difference between the feature data distribution in \\nproduction and training. Prediction drift takes place when feature data distribution in production \\nchanges significantly over time. There is a specific threshold for both skew and drift and once it's \\ncrossed, the model monitor sends an email alert to the admin. TensorFlow Data Validation (TFDV) \\nis an example tool used in model monitoring to compute the distributions and distance scores for \\ndetecting training-serving skew and prediction drift. \\n3.6 Security, Privacy, Governance and Ethical Considerations\\nAlthough Language Model Systems (LLMs) provide remarkable abilities to generate natural \\nlanguage texts based on different inputs like keywords, prompts or queries, they also bring forth \\nsignificant challenges in terms of security, privacy, governance and ethics.\\nLLMs can be vulnerable to various forms of attacks including adversarial examples and \\nmanipulation. Malicious actors might exploit these vulnerabilities to generate misleading or harmful \\ncontent, spread misinformation or engage in other nefarious activities. They can also potentially \\nleak sensitive information from training data or could inadvertently memorize and reproduce private \\nor personally-identifiable details, raising privacy issues for individuals whose data is used in the \\ntraining process.\\nAttacks using prompt injection have emerged as a key vulnerability that affects AI models. \\nPrompt injection attacks can be particularly dangerous for Large Language Models (LLMs) \\nthat use prompt-based learning. These attacks can be executed in different forms depending \\non the system. One method of attack is altering or introducing harmful content into prompts \\nto take advantage of the system. These exploits could make use of real vulnerabilities, modify \\nsystem behavior or trick users. Apart from this, a quick injection attack can be used to cause an \\nunanticipated reaction from LLM-based technologies leading to getting illegal access, influencing  \\noutcomes or going around security precautions. For instance, quick injection attacks frequently \\ntry to steal data in the context of language models. A language model's past instructions may be \\nrevealed through prompt injection, and in some situations, the model may not be able to carry out \\nthose instructions. This enables a malevolent user to circumvent restrictions on what the model is \\npermitted to perform and may potentially reveal sensitive data .\\nThe uncontrolled deployment of LLMs could lead to the generation of biased, offensive or \\ndiscriminatory content. Proper governance mechanisms are necessary to ensure that the outputs \\nof these models align with ethical standards and do not perpetuate harmful biases. LLMs can \\ninadvertently generate content that promotes hate speech, violence or misinformation.\\nSince these models are trained on large and diverse datasets, inherent biases present in the \\ntraining data can lead to biased outputs and since many users may not fully understand how the \\nmodel arrives at specific conclusions, making it difficult to assess the reliability and credibility of the \\ngenerated content as well.\\nGenerating content with LLMs could raise concerns about intellectual property rights. \\nDetermining ownership and authorship of generated texts can be challenging, especially when \\nthe model incorporates a vast corpus of publicly available data.\\n2  |8  |\", metadata={'source': 'llm_doc.pdf', 'page': 9}),\n",
              " Document(page_content='4.\\nSetting Up LLMOps Pipelines\\nA typical LLMOps pipelines need three environments (an environment is a set of isolated compute \\nand associated infrastructure):\\n• Sandbox environment\\n• Runner environment\\n• Production environment\\nUser roles are set up accordingly to manage access to the desired environments.\\nSandbox environment:   \\nEvery LLM initiative starts in a sandbox environment. A sandbox environment can be considered \\nequivalent to a development environment in any software development lifecycle. However, unlike \\na developer’s workstation which is generally used as the development environment, LLMs (or \\nother Deep Learning exercises) require significant computing, multiple experimentation cycles and \\ncollaboration to get to the right recipe. Hence, the sandbox environment itself is hosted on the \\ncloud with LLM hosting capabilities. \\nAs is crucial to any Machine Learning project, quality data is a prerequisite. Hence, setting up a \\ndata foundation with tools for data labeling, data loading and feature storing is the zeroth step.\\nAs discussed in section 3.3 Using Large Language Models - Prompting and Fine-tuning, the first \\nstep in an LLM customizing/building experiment starts with zero-shot/few-shot in-context learning \\nevaluation. This means using a pre-trained model from the LLM hub, providing instructions and \\nexamples for the task at hand as a prompt and evaluating the results. To use a pre-trained model, \\none must first deploy the model for inference within the sandbox environment. Frameworks such as \\nLangchain provide interfaces (prompts, chaining, connectors, etc.) for commonly performed tasks. \\nFine-tuning a prompt (which is debated to be more of an art) can be streamlined with techniques \\nlike re-prompting and auto-prompting.\\nA purely in-context approach may not be suitable for all use cases. To adapt the underlying \\npre-trained LLMs for domain-specific context and task model fine-tuning is the next step in the \\nprocess. With data being the pre-requisite, the focus for fine-tuning is: how efficiently can the \\nmodel be fine tuned?, i.e., minimizing time, dataset size and mix, computing resources while \\nmaximizing model performance on test data.\\nThe result of model fine-tuning and fixing the prompt template is a new model entry to the LLM \\nmodel hub.\\nRunner environment:   \\nThe runner environment ensures successful build and deployment to a target environment.  \\nA successful build requires:\\n• Code checks for vulnerabilities and best practices (such as type match, exception handling)\\n• Code performance optimization\\n|  9', metadata={'source': 'llm_doc.pdf', 'page': 10}),\n",
              " Document(page_content='A successful deployment requires:\\n• The model serves as a REST endpoint for real-time inference OR as a task for  \\n job-based/batch inference\\n• The endpoint can cater to multiple requests  \\n• Logging is enabled \\nIt is integral for CI/CD setup and robust provisioning.  \\nProduction environment:\\nThe production environment hosts the models that integrate the rest of the processes for a  \\nuse case. The distribution of data keeps varying over time. To maintain optimal model performance, \\ntracking the input and output in production is required. Logging and getting the feedback to the \\nsandbox environment for further experimentation is part of this pipeline. \\nLLMs are pre-trained on datasets that can inherently contain various biases. The models  \\ncan end up generating results that include these biases. LLMs are also observed to be highly \\nsensitive to the input prompts. This can result in hallucinated results that are incorrect or  \\nsub-optimal. The sensitivity of the input can also result in a threat of model attacks . \\nHence, the focus in a production environment is to control the quality of input data (content filtering), \\nensure guard rails on model results and monitor for data drifts (part of the observability stack).\\n2  |10  |Figure 1: LLMOps setup and pipelineSandbox Envir onment\\nProduction Envir onmentExperimentation\\nIn-context\\nLearning Data Foundation\\nInference\\nOptimization\\nLLM\\nDeploymentLLM Hub\\nFeedbackRunner\\nEnvir onmentModel\\nUpdating\\nLLM Evaluation\\nLLM Output\\nMonitoring\\nData Pr eparation', metadata={'source': 'llm_doc.pdf', 'page': 11}),\n",
              " Document(page_content='5.\\nConclusions\\nSome benefits of using LLMOps frameworks and platforms are:  \\n• Improved quality and reliability:  LLMOps ensure that LLMs are tested and validated before   \\n deployment, minimizing errors and bugs. LLMOps also monitor and troubleshoot LLMs in   \\n production, ensuring high availability and reliability. \\n• Enhanced scalability and efficiency:  LLMOps optimize the use of computational  \\n resources such as GPUs (Graphics Processing Unit) and TPUs (Tensor Processing Unit) for   \\n LLMs, enabling them to handle large volumes of data and provide real-time predictions.  \\n LLMOps also leverages cloud services and distributed systems for LLMs, enabling them to scale  \\n up or down as needed. Solutions like Ray simplifies the process of deploying large-scale  \\n AI models.\\n• Increased innovation and collaboration:  LLMOps enable data scientists and engineers to   \\n experiment with new ideas and concepts related to LLMs, such as feature engineering, model  \\n architecture and hyperparameters. LLMOps also facilitates communication and coordination   \\n among different stakeholders involved in LLM development and deployment. \\n• Faster time to market:  LLMOps automate and streamline the processes of building, testing   \\n and deploying LLMs, reducing the time and effort required to deliver LLM-powered solutions.\\n• LLM agents:  Agents can help enhance the utility and versatility of LLMs by expanding their   \\n input modalities and action spaces, allowing them to handle complex tasks that involve text,   \\n images, audio, tools and embodiment. This can improve the sociability and trustworthiness   \\n of LLMs by exhibiting natural language communication proficiency, cooperation and negotiation  \\n abilities and explainable decision-making processes.\\nLLMOps is an emerging field that aims to address these challenges and provide best practices \\nand solutions for building and managing large language models. LLMOps can help improve the \\nefficiency, reliability, scalability and reproducibility of NLP projects.\\n6.\\nReferences\\ngoogle/BIG-bench: Beyond the Imitation Game collaborative benchmark for measuring and extrapolating the capabilities of language \\nmodels (github.com)\\nHolistic Evaluation of Language Models (HELM) (stanford.edu)\\nIntroducing FLAN: More generalizable Language Models with Instruction Fine-Tuning\\nBetter Language Models Without Massive Compute\\nDeploying Large NLP Models: Infrastructure Cost Optimization (neptune.ai)\\nBuilding LLM applications for production (huyenchip.com)\\nAlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback\\nSafeguarding and Monitoring Large Language Model (LLM) Applications\\nOptimizing Pre-Trained Models: A Guide to Parameter-Efficient Fine-Tuning (Peft)\\n|  11', metadata={'source': 'llm_doc.pdf', 'page': 12}),\n",
              " Document(page_content='www.mphasis.com\\nNR 27/11/23 US LETTER BASIL8451About Mphasis\\nMphasis’ purpose is to be the “Driver in the Driverless Car”  for Global Enterprises by applying next-generation design, architecture and  \\nengineering services, to deliver scalable and sustainable software and technology solutions. Customer centricity is foundational to Mphasis,  \\nand is reflected in the Mphasis’ Front2Back™ Transformation approach. Front2Back™ uses the exponential power of cloud and cognitive to provide  \\nhyper-personalized (C = X2C²\\n ™= 1) digital experience to clients and their end customers. Mphasis’ Service Transformation approach helps ‘shrink \\nthe core’ through the application of digital technologies across legacy environments within an enterprise, enabling businesses to stay ahead in a \\nchanging world. Mphasis’ core reference architectures and tools, speed and innovation with domain expertise and specialization, combined with an \\nintegrated sustainability and purpose-led approach across its operations and solutions are key to building strong relationships with marquee clients.  \\nClick here  to know more. (BSE: 526299; NSE: MPHASIS)\\nUK\\nMphasis UK Limited\\n1 Ropemaker Street, London\\nEC2Y 9HT, United Kingdom\\nT : +44 020 7153 1327INDIA\\nMphasis Limited \\nBagmane World Technology Center  \\nMarathahalli Ring Road  \\nDoddanakundhi Village, Mahadevapura  \\nBangalore 560 048, India\\nTel.: +91 80 3352 5000For more information, contact: marketinginfo.m@mphasis.com\\nCopyright © Mphasis Corporation. All rights reserved.USA\\nMphasis Corporation\\n41 Madison Avenue\\n35th Floor, New York\\nNew York 10010, USA\\nTel: +1 (212) 686 6655Understanding LLMOps: Large Language Model Operations | Articles – Weights & Biases (wandb.ai)\\nHow Ray Solves Generative AI and LLM Infrastructure Challenges (anyscale.com)\\nPapineni, Kishore, et al. \"Bleu: a method for automatic evaluation of machine translation.\" Proceedings of the 40th annual meeting of \\nthe Association for Computational Linguistics. 2002.\\nLin, Chin-Yew. \"Rouge: A package for automatic evaluation of summaries.\" Text summarization branches out. 2004.\\nMETEOR – Wikipedia\\nZhang, Tianyi, et al. \"Bertscore: Evaluating text generation with bert.\" arXiv preprint arXiv:1904.09675 (2019).\\nRohrbach, Anna, et al. \"Object hallucination in image captioning.\" arXiv preprint arXiv:1809.02156 (2018).\\nLarge Language Model Evaluation in 2023: 5 Methods (aimultiple.com)\\nWei, Jason, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. \\n\"Finetuned language models are zero-shot learners.\" arXiv preprint arXiv:2109.01652  (2021). ([2109.01652] Finetuned Language \\nModels Are Zero-Shot Learners (arxiv.org))\\nYang, N., “Inference with Reference: Lossless Acceleration of Large Language Models”, <i>arXiv e-prints</i>, 2023. doi:10.48550/\\narXiv.2304.04487. ([2304.04487] Inference with Reference: Lossless Acceleration of Large Language Models (arxiv.org))\\nDeploying Large NLP Models: Infrastructure Cost Optimization (neptune.ai)\\nTrain 175+ billion parameter NLP models with model parallel additions and Hugging Face on Amazon SageMaker | AWS Machine \\nLearning BlogLangchain (Introduction | Langchain))', metadata={'source': 'llm_doc.pdf', 'page': 13})]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcyIP8vUf8pn"
      },
      "source": [
        "## 1.2. Generación de 'chunks'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ps7um1CM1lpL"
      },
      "outputs": [],
      "source": [
        "# Esta clase se utiliza para dividir textos en fragmentos más pequeños, basándose en el número de caracteres.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Se especifica el tamaño de cada fragmento de texto (chunk_size) en 1000 caracteres y\n",
        "# la superposición entre fragmentos consecutivos (chunk_overlap) en 100 caracteres.\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size    = 1000,\n",
        "                                               chunk_overlap = 100)\n",
        "\n",
        "# Este método divide el contenido de los documentos cargados en fragmentos más pequeños,\n",
        "# basándose en los parámetros de tamaño y superposición especificados.\n",
        "doc_splits = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "iuiQATZ42bd6",
        "outputId": "b1a3d801-7929-4328-869c-50562243f141"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1.\\nIntroduction to LLMOps\\nGenerative AI models have gained wide popularity in recent times with the adoption of  \\ntransformer-based neural network architectures. Generative model’s ability to generate new data \\nenables them to go beyond traditional prediction and classification use cases. These models \\nare now used across domains and use cases like chatbots, question answering, fraud detection, \\nprotein folding and many more.\\nGenerative AI models for natural language use cases are powered by Large Language Models (LLMs). \\nLLMs are transformer-based Deep Learning architectures that harness vast amounts of textual \\ndata to develop language and domain understanding. The models are built with an emphasis on \\ngenerating human-like responses and reasoning. Their ability to understand human languages allows \\nthem to serve as powerful tools for information retrieval, natural language processing, language \\ntranslation and even creative writing.'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc_splits[2].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "MaE36ZsriRtq",
        "outputId": "e03244d4-3010-45b7-e476-3656764b11e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'translation and even creative writing.\\nUsing large language models in production environments poses a certain unique set of challenges \\nsuch as organizing LLMs into agents for sub-tasks, developing robust instructions for each  \\nLLM agent, evaluating the correctness of generated response and efficiencies with fine-tuning.  \\nHence, effective usage in a production environment requires appropriate infrastructure and practices \\nfocusing on experimentation, deployment, management and monitoring of large language models. \\nLarge Language Model Operations (LLMOps) is a framework of tools and best practices  \\nto manage the lifecycle of LLM-powered applications, from development to deployment  \\nand maintenance.\\nThe aim is to enable AI capabilities with LLMs by developing better prompts, longer context, \\nfaster inference and customized techniques that enable rapid experimentation and innovation \\nwith LLMs. Together, these allow data scientists and engineers to collaborate effectively and'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc_splits[3].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n47-kv22eMw",
        "outputId": "e65debf2-856f-4ee4-e23a-2b2652b837d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source': 'llm_doc.pdf', 'page': 2}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc_splits[3].metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TwUdY3GgMti"
      },
      "source": [
        "## 1.3. Crear Vector Store como dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZdK8v8_S2CRu",
        "outputId": "abc94f0b-7075-4d15-bb8f-afb2887c1dee"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Chunks</th>\n",
              "      <th>Metadata</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Building Pipelines and Environments for  \\nLar...</td>\n",
              "      <td>{'source': 'llm_doc.pdf', 'page': 0}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Contents\\nIntroduction to LLMOps 1\\nWhy LLMOps...</td>\n",
              "      <td>{'source': 'llm_doc.pdf', 'page': 1}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.\\nIntroduction to LLMOps\\nGenerative AI mode...</td>\n",
              "      <td>{'source': 'llm_doc.pdf', 'page': 2}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>translation and even creative writing.\\nUsing ...</td>\n",
              "      <td>{'source': 'llm_doc.pdf', 'page': 2}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>with LLMs. Together, these allow data scientis...</td>\n",
              "      <td>{'source': 'llm_doc.pdf', 'page': 2}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Chunks  \\\n",
              "0  Building Pipelines and Environments for  \\nLar...   \n",
              "1  Contents\\nIntroduction to LLMOps 1\\nWhy LLMOps...   \n",
              "2  1.\\nIntroduction to LLMOps\\nGenerative AI mode...   \n",
              "3  translation and even creative writing.\\nUsing ...   \n",
              "4  with LLMs. Together, these allow data scientis...   \n",
              "\n",
              "                               Metadata  \n",
              "0  {'source': 'llm_doc.pdf', 'page': 0}  \n",
              "1  {'source': 'llm_doc.pdf', 'page': 1}  \n",
              "2  {'source': 'llm_doc.pdf', 'page': 2}  \n",
              "3  {'source': 'llm_doc.pdf', 'page': 2}  \n",
              "4  {'source': 'llm_doc.pdf', 'page': 2}  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# Crear una lista de diccionarios a partir de los fragmentos de documento divididos previamente.\n",
        "# Cada diccionario contiene dos claves: 'Chunks', que almacena el contenido del fragmento de texto,\n",
        "# y 'Metadata', que almacena los metadatos asociados a dicho fragmento.\n",
        "data = [{'Chunks': doc.page_content, 'Metadata': doc.metadata} for doc in doc_splits]\n",
        "# Crear un DataFrame de pandas a partir de la lista de diccionarios 'data'.\n",
        "df_vector_store = pd.DataFrame(data)\n",
        "df_vector_store.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ituYrA4H4hv0",
        "outputId": "ac2101ff-e035-4076-b635-3a1389b8c464"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Chunks</th>\n",
              "      <th>Metadata</th>\n",
              "      <th>Embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Building Pipelines and Environments for  \\nLar...</td>\n",
              "      <td>{'source': 'llm_doc.pdf', 'page': 0}</td>\n",
              "      <td>[-0.00042193758, -0.012498714, 0.017564613, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Contents\\nIntroduction to LLMOps 1\\nWhy LLMOps...</td>\n",
              "      <td>{'source': 'llm_doc.pdf', 'page': 1}</td>\n",
              "      <td>[0.0030914678, -0.0044048806, -0.002168892, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.\\nIntroduction to LLMOps\\nGenerative AI mode...</td>\n",
              "      <td>{'source': 'llm_doc.pdf', 'page': 2}</td>\n",
              "      <td>[-0.021246167, -0.008550943, -0.013926398, -0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>translation and even creative writing.\\nUsing ...</td>\n",
              "      <td>{'source': 'llm_doc.pdf', 'page': 2}</td>\n",
              "      <td>[-0.006791799, -0.0054946076, -0.0034398423, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>with LLMs. Together, these allow data scientis...</td>\n",
              "      <td>{'source': 'llm_doc.pdf', 'page': 2}</td>\n",
              "      <td>[0.007078849, -0.010141378, 0.003159643, -0.05...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Chunks  \\\n",
              "0  Building Pipelines and Environments for  \\nLar...   \n",
              "1  Contents\\nIntroduction to LLMOps 1\\nWhy LLMOps...   \n",
              "2  1.\\nIntroduction to LLMOps\\nGenerative AI mode...   \n",
              "3  translation and even creative writing.\\nUsing ...   \n",
              "4  with LLMs. Together, these allow data scientis...   \n",
              "\n",
              "                               Metadata  \\\n",
              "0  {'source': 'llm_doc.pdf', 'page': 0}   \n",
              "1  {'source': 'llm_doc.pdf', 'page': 1}   \n",
              "2  {'source': 'llm_doc.pdf', 'page': 2}   \n",
              "3  {'source': 'llm_doc.pdf', 'page': 2}   \n",
              "4  {'source': 'llm_doc.pdf', 'page': 2}   \n",
              "\n",
              "                                           Embedding  \n",
              "0  [-0.00042193758, -0.012498714, 0.017564613, -0...  \n",
              "1  [0.0030914678, -0.0044048806, -0.002168892, -0...  \n",
              "2  [-0.021246167, -0.008550943, -0.013926398, -0....  \n",
              "3  [-0.006791799, -0.0054946076, -0.0034398423, -...  \n",
              "4  [0.007078849, -0.010141378, 0.003159643, -0.05...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Importar la clase OpenAI del módulo openai. Esta clase proporciona acceso a las API de OpenAI.\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "import json\n",
        "# Abrir el archivo 'credentials.json' en modo de lectura.\n",
        "file_name = open('credentials.json')\n",
        "# Cargar el contenido del archivo JSON en una variable llamada 'config_env'.\n",
        "config_env = json.load(file_name)\n",
        "# Extraer el valor asociado a la clave 'openai_key' del diccionario 'config_env'.\n",
        "api_key = config_env[\"openai_key\"]\n",
        "# Crear una instancia de la clase OpenAI, proporcionando la clave API extraída del archivo de configuración.\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# Llamar al método 'embeddings.create' del cliente de OpenAI para crear embeddings del texto.\n",
        "# Se especifica el modelo 'text-embedding-ada-002', el texto de entrada y el formato de codificación.\n",
        "# La función retorna la representación vectorial del texto.\n",
        "def text_embedding(text=[]):\n",
        "    embeddings = client.embeddings.create(model=\"text-embedding-ada-002\",\n",
        "                                          input=text,\n",
        "                                          encoding_format=\"float\")\n",
        "    return embeddings.data[0].embedding\n",
        "\n",
        "# Añadir una nueva columna 'Embedding' al DataFrame 'df_vector_store'.\n",
        "df_vector_store[\"Embedding\"] = df_vector_store[\"Chunks\"].apply(lambda x: text_embedding([x]))\n",
        "df_vector_store[\"Embedding\"] = df_vector_store[\"Embedding\"].apply(np.array)\n",
        "\n",
        "# Guardar el DataFrame 'df_vector_store' en un archivo con formato pickle.\n",
        "df_vector_store.to_pickle('df_vector_store.pkl')\n",
        "df_vector_store.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc5XZd_lhdey"
      },
      "source": [
        "# 2.  Formulación de pregunta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SSrfqvArhd3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-0.02419639,\n",
              " 0.011456056,\n",
              " 0.0020118116,\n",
              " -0.02696989,\n",
              " -0.041670803,\n",
              " 0.028281495,\n",
              " -0.026204787,\n",
              " 0.024128078,\n",
              " -0.010930046,\n",
              " -0.023649888,\n",
              " 0.017911615,\n",
              " 0.025945198,\n",
              " -0.012064039,\n",
              " 0.0051200436,\n",
              " -0.02553532,\n",
              " -0.0005460754,\n",
              " -0.0007160035,\n",
              " -0.0006946557,\n",
              " 0.026723964,\n",
              " -0.009092432,\n",
              " -0.0063974927,\n",
              " -0.012945274,\n",
              " 0.028008245,\n",
              " -0.031314585,\n",
              " -0.0047614016,\n",
              " 0.0010187145,\n",
              " 0.019687746,\n",
              " -0.013792353,\n",
              " 0.0022013797,\n",
              " -0.001734291,\n",
              " 0.021477541,\n",
              " 0.024059765,\n",
              " -0.026997216,\n",
              " -0.019400833,\n",
              " -0.019428158,\n",
              " 0.00843663,\n",
              " 0.01617647,\n",
              " -0.003828932,\n",
              " 0.018376142,\n",
              " -0.0077535016,\n",
              " 0.024606267,\n",
              " 0.01409976,\n",
              " 0.014946839,\n",
              " -0.0071113617,\n",
              " 0.021545855,\n",
              " -0.005857822,\n",
              " -0.017091861,\n",
              " 0.0025002481,\n",
              " -0.03391047,\n",
              " -0.0010246919,\n",
              " -0.019455483,\n",
              " 0.014946839,\n",
              " -0.020439187,\n",
              " -0.007828646,\n",
              " 0.0052771633,\n",
              " 0.0039860513,\n",
              " -0.0049902494,\n",
              " -0.004006545,\n",
              " 0.021819105,\n",
              " -0.012863299,\n",
              " -0.0022936019,\n",
              " -0.012098195,\n",
              " -0.0121118575,\n",
              " 0.016804947,\n",
              " 0.006424818,\n",
              " 0.00070831826,\n",
              " 0.0005144807,\n",
              " 0.0010084676,\n",
              " 0.020357212,\n",
              " 0.0037503722,\n",
              " 0.0014063896,\n",
              " -0.01815754,\n",
              " 0.007917453,\n",
              " -0.014181736,\n",
              " 0.020097623,\n",
              " 0.0049458463,\n",
              " -0.022092357,\n",
              " -0.009495478,\n",
              " 0.0014388382,\n",
              " -0.008354655,\n",
              " 1.9039522e-05,\n",
              " -0.032434914,\n",
              " -0.035112776,\n",
              " 0.031150633,\n",
              " 0.020384537,\n",
              " -0.0006865435,\n",
              " 0.0295931,\n",
              " 0.008108729,\n",
              " -0.02601351,\n",
              " -0.020220587,\n",
              " -0.025562646,\n",
              " 0.002273108,\n",
              " 0.05667229,\n",
              " -0.005847575,\n",
              " -0.013498608,\n",
              " 0.009748235,\n",
              " 0.0016617087,\n",
              " 0.0006190847,\n",
              " 0.0026419973,\n",
              " -0.013177537,\n",
              " 0.0014337148,\n",
              " 0.008552762,\n",
              " -0.04811953,\n",
              " 0.0009025828,\n",
              " -0.01271301,\n",
              " -0.004932184,\n",
              " -0.009071939,\n",
              " 0.020425525,\n",
              " -0.0051815254,\n",
              " -0.02195573,\n",
              " -0.024551617,\n",
              " 0.031314585,\n",
              " -0.0016651243,\n",
              " -0.052354924,\n",
              " 0.0067219785,\n",
              " -0.012863299,\n",
              " -0.022652522,\n",
              " -0.020193262,\n",
              " -0.0061379042,\n",
              " -0.016203795,\n",
              " 0.020534825,\n",
              " 0.020111285,\n",
              " 0.019168569,\n",
              " -0.03710751,\n",
              " 0.021709805,\n",
              " -0.009324696,\n",
              " -0.023431288,\n",
              " -0.018867994,\n",
              " -0.034047097,\n",
              " -0.0010400623,\n",
              " 0.014878526,\n",
              " 0.029975653,\n",
              " -0.010595314,\n",
              " 0.02419639,\n",
              " -0.012528566,\n",
              " 0.01142873,\n",
              " -0.013648896,\n",
              " 0.02792627,\n",
              " -0.039129566,\n",
              " 0.0019793632,\n",
              " 0.022228982,\n",
              " -0.0016266983,\n",
              " -0.010759264,\n",
              " 0.009249552,\n",
              " 0.02199672,\n",
              " 0.019578446,\n",
              " 0.01927787,\n",
              " -0.01084124,\n",
              " -0.018143877,\n",
              " 0.005403542,\n",
              " -0.017037211,\n",
              " -0.023540588,\n",
              " 0.00568704,\n",
              " 0.009905355,\n",
              " 0.0203982,\n",
              " 0.020931039,\n",
              " 0.009946343,\n",
              " 0.012624204,\n",
              " -0.018171202,\n",
              " -0.017242149,\n",
              " 0.012487578,\n",
              " -0.015793918,\n",
              " -0.0046316073,\n",
              " 0.017911615,\n",
              " 0.019004619,\n",
              " 0.033719193,\n",
              " 0.03380117,\n",
              " -0.018143877,\n",
              " -0.0026829848,\n",
              " 0.015302065,\n",
              " -0.009768729,\n",
              " 0.03273549,\n",
              " -0.027133841,\n",
              " 0.032926764,\n",
              " 0.0028554746,\n",
              " 0.00483313,\n",
              " 0.010923215,\n",
              " -0.034183722,\n",
              " -0.0020408446,\n",
              " -0.014195398,\n",
              " -0.019127581,\n",
              " 0.03038553,\n",
              " -0.011585849,\n",
              " 0.019578446,\n",
              " -0.042818457,\n",
              " 0.012398772,\n",
              " -0.01634042,\n",
              " 0.0008171918,\n",
              " 0.0021176965,\n",
              " -0.0031133555,\n",
              " -0.0034139317,\n",
              " 0.017665688,\n",
              " 0.014263711,\n",
              " -0.008641568,\n",
              " -0.62344986,\n",
              " -0.011674656,\n",
              " -0.0052156816,\n",
              " -0.012419266,\n",
              " 0.02232462,\n",
              " 0.01767935,\n",
              " 0.008054079,\n",
              " -0.0073572877,\n",
              " -0.013211694,\n",
              " -0.049677063,\n",
              " -0.011538031,\n",
              " -0.009509141,\n",
              " 0.026396062,\n",
              " -0.0141407475,\n",
              " -0.02226997,\n",
              " -0.034538947,\n",
              " 0.007459757,\n",
              " -0.013594246,\n",
              " 0.0032773062,\n",
              " 0.034894172,\n",
              " -0.0021740545,\n",
              " -0.027133841,\n",
              " 0.00086757244,\n",
              " 0.022488572,\n",
              " -0.0011988895,\n",
              " 0.014659925,\n",
              " 0.002378993,\n",
              " -0.008306836,\n",
              " -0.0046111136,\n",
              " 0.0033729442,\n",
              " -0.019769723,\n",
              " 0.002387532,\n",
              " -0.017802313,\n",
              " 0.00881235,\n",
              " 0.045359693,\n",
              " -0.021340916,\n",
              " -0.026245775,\n",
              " 0.023895815,\n",
              " -0.0002899024,\n",
              " 0.030194255,\n",
              " -0.02783063,\n",
              " -0.016108157,\n",
              " 0.010554327,\n",
              " -0.010697783,\n",
              " 0.011175972,\n",
              " 0.0044027595,\n",
              " 0.005249838,\n",
              " -0.0044369157,\n",
              " 0.008211197,\n",
              " 0.016094493,\n",
              " 0.008375148,\n",
              " -0.00055034494,\n",
              " -0.014605274,\n",
              " 0.006445312,\n",
              " 0.034620922,\n",
              " -0.015821243,\n",
              " 0.0355773,\n",
              " -0.034484297,\n",
              " 0.005895394,\n",
              " 0.00011154198,\n",
              " -0.021819105,\n",
              " -0.013122887,\n",
              " -0.030686107,\n",
              " -0.019031944,\n",
              " 0.007678358,\n",
              " 0.0262731,\n",
              " 0.018813342,\n",
              " 0.011599512,\n",
              " 0.011025685,\n",
              " -0.011073504,\n",
              " -0.011531199,\n",
              " 0.026122812,\n",
              " -0.012344121,\n",
              " 0.0138128465,\n",
              " -0.013184369,\n",
              " -0.001231338,\n",
              " -0.0048058047,\n",
              " 0.017597375,\n",
              " 0.018130215,\n",
              " -0.019455483,\n",
              " 0.007125024,\n",
              " -0.012535397,\n",
              " -0.00091966096,\n",
              " -0.0086552305,\n",
              " 0.005772431,\n",
              " -0.016900584,\n",
              " -0.017856963,\n",
              " -0.023677213,\n",
              " -0.006209633,\n",
              " 0.03347327,\n",
              " 0.015766593,\n",
              " 0.009632103,\n",
              " -0.015192765,\n",
              " -0.011954739,\n",
              " -0.020247912,\n",
              " -0.0040611955,\n",
              " -0.014318361,\n",
              " 0.023076061,\n",
              " 0.016914247,\n",
              " -0.033200018,\n",
              " -0.002226997,\n",
              " -0.0075280694,\n",
              " 0.017501738,\n",
              " 0.004453994,\n",
              " 0.025726598,\n",
              " 0.010561158,\n",
              " -0.01634042,\n",
              " 0.0029903925,\n",
              " 0.024018778,\n",
              " -0.027584706,\n",
              " -0.0030450427,\n",
              " 0.012829143,\n",
              " -0.014933176,\n",
              " 0.004218315,\n",
              " 0.01190692,\n",
              " -0.029210549,\n",
              " -0.011298936,\n",
              " 0.009563791,\n",
              " 0.010800253,\n",
              " 0.0030843224,\n",
              " 0.009584284,\n",
              " -0.0021091574,\n",
              " 0.03038553,\n",
              " 0.0140860975,\n",
              " -0.00043848273,\n",
              " 0.01142873,\n",
              " -0.008853338,\n",
              " 0.014946839,\n",
              " 0.008484448,\n",
              " 0.004949262,\n",
              " 0.007616876,\n",
              " -0.0120845325,\n",
              " 0.044840515,\n",
              " -0.021272603,\n",
              " 0.03273549,\n",
              " 0.009119757,\n",
              " 0.01313655,\n",
              " 0.0013858958,\n",
              " 0.012050376,\n",
              " 0.0039928826,\n",
              " 0.0005934674,\n",
              " 0.0020732933,\n",
              " -0.0041739116,\n",
              " -0.01479655,\n",
              " -0.011148647,\n",
              " -0.037763312,\n",
              " -0.016299432,\n",
              " 0.014659925,\n",
              " -0.011346755,\n",
              " -0.0065136245,\n",
              " -0.016832272,\n",
              " -0.024264704,\n",
              " -0.0292652,\n",
              " 0.022611534,\n",
              " -0.0043617715,\n",
              " 0.006698069,\n",
              " -0.030084953,\n",
              " -0.029784378,\n",
              " -0.00691667,\n",
              " -0.008074572,\n",
              " 0.01121696,\n",
              " 0.016313095,\n",
              " -0.021983055,\n",
              " 0.015616304,\n",
              " -0.005710949,\n",
              " -0.017433424,\n",
              " 0.017091861,\n",
              " 0.023759188,\n",
              " -0.020247912,\n",
              " -0.009502309,\n",
              " 0.013109225,\n",
              " -0.009529634,\n",
              " -0.0058031715,\n",
              " 0.024210053,\n",
              " -0.0032294872,\n",
              " -0.012439759,\n",
              " 0.01644972,\n",
              " -0.017870625,\n",
              " 0.0018922644,\n",
              " 0.003545434,\n",
              " -0.00106141,\n",
              " 0.031068658,\n",
              " 0.018403467,\n",
              " 0.012651529,\n",
              " 0.0074460944,\n",
              " 0.014332023,\n",
              " -0.015588979,\n",
              " 0.02467458,\n",
              " -0.032653514,\n",
              " 0.027147504,\n",
              " 0.01324585,\n",
              " -0.0014755563,\n",
              " 0.013423463,\n",
              " 0.001193766,\n",
              " 0.024647254,\n",
              " 0.021819105,\n",
              " 0.017187499,\n",
              " -0.0031987464,\n",
              " 0.00058578217,\n",
              " -0.008470786,\n",
              " -0.016545357,\n",
              " 0.006387246,\n",
              " 0.026519025,\n",
              " -0.022488572,\n",
              " 0.0033900223,\n",
              " -0.030084953,\n",
              " 0.009905355,\n",
              " 0.0020920793,\n",
              " 0.016094493,\n",
              " 0.037134834,\n",
              " 0.021272603,\n",
              " -0.017173836,\n",
              " -0.0010571404,\n",
              " -0.003945064,\n",
              " 0.01121696,\n",
              " 0.02109499,\n",
              " -0.0002758129,\n",
              " -0.004047533,\n",
              " -0.008074572,\n",
              " 0.03251689,\n",
              " -0.008750869,\n",
              " 0.011852269,\n",
              " 0.026204787,\n",
              " -0.0295931,\n",
              " 0.017583713,\n",
              " -0.00028008246,\n",
              " -0.0016642704,\n",
              " 0.010322063,\n",
              " 0.0011203298,\n",
              " -0.010561158,\n",
              " -0.02056215,\n",
              " -0.012719842,\n",
              " 0.009522803,\n",
              " 0.015629966,\n",
              " -0.006793707,\n",
              " -0.008559593,\n",
              " 0.0039211544,\n",
              " 0.0073504564,\n",
              " 0.019960998,\n",
              " -0.012590048,\n",
              " 0.012849636,\n",
              " 0.03273549,\n",
              " -0.012890624,\n",
              " 0.0089284815,\n",
              " 0.03552265,\n",
              " -0.01639507,\n",
              " 0.019168569,\n",
              " 0.028500097,\n",
              " 0.005833912,\n",
              " 0.002899878,\n",
              " -0.031232608,\n",
              " 0.021190628,\n",
              " -0.030576805,\n",
              " 0.005782678,\n",
              " 0.012856468,\n",
              " -0.01597153,\n",
              " -0.0015848568,\n",
              " -0.006882514,\n",
              " -0.0008197535,\n",
              " 0.0009059984,\n",
              " 0.013915315,\n",
              " 0.033992443,\n",
              " 0.005782678,\n",
              " -0.011879594,\n",
              " 0.008935313,\n",
              " -0.022625197,\n",
              " -0.00502099,\n",
              " -0.015288402,\n",
              " 0.01281548,\n",
              " -0.006670744,\n",
              " -0.006663913,\n",
              " 0.008245354,\n",
              " 0.02366355,\n",
              " -0.017706675,\n",
              " 0.011524368,\n",
              " -0.012460253,\n",
              " 0.00881235,\n",
              " -0.005659715,\n",
              " 0.021013014,\n",
              " 0.03683426,\n",
              " -0.023185361,\n",
              " -0.01960577,\n",
              " 0.034047097,\n",
              " 0.0141680725,\n",
              " 0.0031594667,\n",
              " -0.016955234,\n",
              " -0.020903714,\n",
              " 0.008006259,\n",
              " 0.0036547342,\n",
              " -0.0067151473,\n",
              " 0.019783385,\n",
              " -0.016750297,\n",
              " -0.00012264281,\n",
              " -0.0064214026,\n",
              " -0.009625272,\n",
              " 0.033200018,\n",
              " 0.041206274,\n",
              " -0.003562512,\n",
              " -0.02809022,\n",
              " -0.0021313592,\n",
              " -0.009372515,\n",
              " 0.00437885,\n",
              " -0.021491203,\n",
              " -0.02964775,\n",
              " 0.026232112,\n",
              " 0.017255811,\n",
              " -0.019851698,\n",
              " -0.0121938335,\n",
              " 0.019045606,\n",
              " -0.017597375,\n",
              " 0.0003242723,\n",
              " 0.00046281915,\n",
              " 0.00397922,\n",
              " -0.016996222,\n",
              " 0.0054445295,\n",
              " -0.0042695496,\n",
              " -0.0011305767,\n",
              " 0.008935313,\n",
              " 0.028609397,\n",
              " 0.00555383,\n",
              " -0.004358356,\n",
              " -0.02414174,\n",
              " -0.007855971,\n",
              " -0.016367745,\n",
              " -0.009106095,\n",
              " 0.025890548,\n",
              " 0.013498608,\n",
              " 0.0068654357,\n",
              " -0.02584956,\n",
              " -0.023226349,\n",
              " -0.0060832542,\n",
              " -0.004494982,\n",
              " 0.011654162,\n",
              " 0.034784872,\n",
              " -0.02568561,\n",
              " -0.0039382325,\n",
              " 0.033828493,\n",
              " 0.015875893,\n",
              " 0.0036888907,\n",
              " -0.0033422033,\n",
              " 0.025453346,\n",
              " -0.011093997,\n",
              " -0.03910224,\n",
              " -0.017597375,\n",
              " -0.007343625,\n",
              " -0.019414496,\n",
              " 0.008730375,\n",
              " 0.02809022,\n",
              " 0.00016757981,\n",
              " -0.002681277,\n",
              " -0.0015370378,\n",
              " -0.001715505,\n",
              " 0.0022526144,\n",
              " -0.015930543,\n",
              " 0.0085732555,\n",
              " 0.00547527,\n",
              " 0.018922644,\n",
              " 0.024264704,\n",
              " 0.030467505,\n",
              " 0.03358257,\n",
              " -0.014250048,\n",
              " -0.0065546124,\n",
              " 0.02173713,\n",
              " 0.010240087,\n",
              " 0.022898447,\n",
              " 0.0012962352,\n",
              " 0.010322063,\n",
              " 0.015315728,\n",
              " 0.0019742397,\n",
              " -0.0040509487,\n",
              " 0.01650437,\n",
              " 0.007466588,\n",
              " 0.020644126,\n",
              " -0.023458613,\n",
              " -0.0053420602,\n",
              " -0.017597375,\n",
              " -0.04511377,\n",
              " -0.00095467124,\n",
              " 0.014946839,\n",
              " 0.035850555,\n",
              " -0.004348109,\n",
              " -0.01767935,\n",
              " -0.0140587725,\n",
              " -0.019469146,\n",
              " -0.02819952,\n",
              " -0.0010059058,\n",
              " 0.018717704,\n",
              " -0.0063155177,\n",
              " -0.003022841,\n",
              " 0.0033165861,\n",
              " -0.017474413,\n",
              " 0.009160745,\n",
              " -0.039703395,\n",
              " -0.0121665085,\n",
              " -0.00091026793,\n",
              " -0.015561654,\n",
              " -0.068203494,\n",
              " -0.006814201,\n",
              " 0.012938443,\n",
              " 0.009980499,\n",
              " 0.020097623,\n",
              " 0.016094493,\n",
              " 0.0062984396,\n",
              " 0.018990956,\n",
              " -0.018936306,\n",
              " 0.023909478,\n",
              " 0.01303408,\n",
              " -0.018389804,\n",
              " 0.0029118326,\n",
              " 0.018362477,\n",
              " 0.0008188996,\n",
              " -0.0046589323,\n",
              " -0.0014507929,\n",
              " 0.018389804,\n",
              " -0.00600811,\n",
              " 0.017228486,\n",
              " 0.011797619,\n",
              " -0.010854903,\n",
              " -0.008443461,\n",
              " 0.02658734,\n",
              " 0.01105301,\n",
              " 0.008443461,\n",
              " -0.0042593027,\n",
              " -0.020370875,\n",
              " 0.017160174,\n",
              " -0.027816968,\n",
              " -0.028691372,\n",
              " -0.008054079,\n",
              " -0.019045606,\n",
              " 0.008976301,\n",
              " 0.013368813,\n",
              " 0.006984983,\n",
              " -0.01944182,\n",
              " 0.005386464,\n",
              " -0.00023674652,\n",
              " -0.004843377,\n",
              " 0.023800176,\n",
              " -0.003668397,\n",
              " -0.0041500023,\n",
              " 0.029511126,\n",
              " 0.018827006,\n",
              " -0.0020732933,\n",
              " 0.0061071636,\n",
              " -0.028718697,\n",
              " 0.025507996,\n",
              " -0.05281945,\n",
              " 0.042982407,\n",
              " 0.03727146,\n",
              " -0.017064536,\n",
              " 0.018499104,\n",
              " -0.01821219,\n",
              " -0.020862726,\n",
              " -0.00046794262,\n",
              " 0.014454987,\n",
              " 0.0059090564,\n",
              " 0.041944053,\n",
              " -0.00838198,\n",
              " 0.012685685,\n",
              " -0.03519475,\n",
              " 0.0004576957,\n",
              " -0.028226845,\n",
              " 0.031068658,\n",
              " -0.020753426,\n",
              " -0.0020527993,\n",
              " 0.007172843,\n",
              " -0.00096406427,\n",
              " -0.017351449,\n",
              " -0.0121118575,\n",
              " -0.0026659067,\n",
              " -0.022994086,\n",
              " 0.0070020612,\n",
              " 0.043501586,\n",
              " 0.0036991376,\n",
              " 0.02931985,\n",
              " -0.0073572877,\n",
              " 0.02531672,\n",
              " -0.043446936,\n",
              " -0.047026526,\n",
              " 0.0059432127,\n",
              " 0.0053420602,\n",
              " 0.010103462,\n",
              " -0.014659925,\n",
              " 0.024537954,\n",
              " 0.016094493,\n",
              " 0.028281495,\n",
              " 0.026355075,\n",
              " 0.014591612,\n",
              " -0.028527422,\n",
              " -0.020644126,\n",
              " 0.0120845325,\n",
              " 0.025439683,\n",
              " 0.0071045305,\n",
              " 0.0056699617,\n",
              " 0.026095487,\n",
              " 0.019988323,\n",
              " -0.030658782,\n",
              " 0.027147504,\n",
              " -0.003647903,\n",
              " -0.0029681907,\n",
              " 0.005980785,\n",
              " -0.027297791,\n",
              " 0.013163875,\n",
              " -0.020316225,\n",
              " -0.014482312,\n",
              " 0.0105201695,\n",
              " -0.015998855,\n",
              " 0.006578522,\n",
              " -0.0065990156,\n",
              " -0.024633592,\n",
              " 0.008648399,\n",
              " 0.004416422,\n",
              " 0.0018239515,\n",
              " 0.00907877,\n",
              " 0.008593749,\n",
              " 0.000426528,\n",
              " -0.02686059,\n",
              " 0.033445943,\n",
              " -0.0030365037,\n",
              " 0.0068961764,\n",
              " -0.0016864721,\n",
              " -0.02157318,\n",
              " -0.037681337,\n",
              " 0.009406671,\n",
              " 0.013628402,\n",
              " -0.01025375,\n",
              " 0.010608977,\n",
              " 0.008976301,\n",
              " -0.015766593,\n",
              " 0.009632103,\n",
              " -0.033773843,\n",
              " 0.0074734194,\n",
              " -0.0059602913,\n",
              " -0.005492348,\n",
              " -0.01628577,\n",
              " -0.008593749,\n",
              " -0.048037555,\n",
              " -0.0050483155,\n",
              " -0.009679923,\n",
              " 0.01681861,\n",
              " 0.015179102,\n",
              " 0.007978934,\n",
              " 0.0025446515,\n",
              " -0.0120298825,\n",
              " -0.026669314,\n",
              " 0.009372515,\n",
              " -0.019879023,\n",
              " 0.04240858,\n",
              " -0.0055674925,\n",
              " 0.0121391835,\n",
              " 0.010342557,\n",
              " -0.0029579438,\n",
              " -0.025043469,\n",
              " 0.013560089,\n",
              " -0.0060149413,\n",
              " 0.009502309,\n",
              " 0.0036786438,\n",
              " 0.029675076,\n",
              " 0.014427662,\n",
              " -0.023431288,\n",
              " 0.008231691,\n",
              " -0.00063189334,\n",
              " -0.003489076,\n",
              " -0.023089724,\n",
              " 0.024469642,\n",
              " 0.010656795,\n",
              " -0.009550128,\n",
              " 0.0018034577,\n",
              " -0.010028318,\n",
              " -0.01681861,\n",
              " 0.001459332,\n",
              " 0.004416422,\n",
              " -0.0011109367,\n",
              " 0.021764455,\n",
              " -0.0141680725,\n",
              " 0.013532764,\n",
              " 0.005410373,\n",
              " -0.005516258,\n",
              " 0.0053898795,\n",
              " -0.011353586,\n",
              " -0.021040339,\n",
              " 0.017392436,\n",
              " -0.028008245,\n",
              " -0.019687746,\n",
              " 0.0295931,\n",
              " 0.0053044884,\n",
              " 0.026573677,\n",
              " 0.0072343247,\n",
              " 0.0121938335,\n",
              " -0.004880949,\n",
              " -0.01388799,\n",
              " 0.008156547,\n",
              " 0.011538031,\n",
              " 0.016586347,\n",
              " 0.023171699,\n",
              " -0.006206217,\n",
              " -0.01778865,\n",
              " 0.0033712361,\n",
              " -0.00020835402,\n",
              " -0.0057587684,\n",
              " 0.0019776553,\n",
              " -0.012774492,\n",
              " 0.008696218,\n",
              " -0.0026795692,\n",
              " -0.009440828,\n",
              " 0.011059841,\n",
              " 0.023144374,\n",
              " -0.016477045,\n",
              " -0.0011630253,\n",
              " -0.0087918565,\n",
              " -0.031751785,\n",
              " -0.019045606,\n",
              " 0.008422967,\n",
              " -0.024947831,\n",
              " 0.04470389,\n",
              " 0.009468153,\n",
              " -0.00929054,\n",
              " 0.00086928025,\n",
              " 0.023458613,\n",
              " -0.0008586064,\n",
              " 0.0042012366,\n",
              " 0.022939436,\n",
              " 0.0023721617,\n",
              " -0.0236089,\n",
              " 0.012412434,\n",
              " -0.009222227,\n",
              " -0.021067664,\n",
              " -0.0013995583,\n",
              " 0.017652025,\n",
              " 0.0064487276,\n",
              " -0.020903714,\n",
              " 0.019127581,\n",
              " 0.015466016,\n",
              " -0.014605274,\n",
              " -0.016217457,\n",
              " 0.008040415,\n",
              " -0.005622143,\n",
              " -0.00028200375,\n",
              " -0.01778865,\n",
              " -0.012323627,\n",
              " 0.009994161,\n",
              " 0.019988323,\n",
              " -0.0028981701,\n",
              " -0.0019827788,\n",
              " 0.022037707,\n",
              " -0.01841713,\n",
              " 0.0048467927,\n",
              " -0.0017966264,\n",
              " 0.0030023472,\n",
              " -0.003945064,\n",
              " -0.0014089514,\n",
              " -0.010608977,\n",
              " -0.0031423883,\n",
              " -0.025835898,\n",
              " 0.0017086737,\n",
              " -0.017542725,\n",
              " 0.0027598368,\n",
              " -0.00176247,\n",
              " -0.01569828,\n",
              " -0.033254668,\n",
              " -0.016777622,\n",
              " -0.0089011565,\n",
              " 0.013204862,\n",
              " -0.0060217725,\n",
              " -0.0005435136,\n",
              " 0.0029408657,\n",
              " 0.017611038,\n",
              " 0.012200665,\n",
              " 0.038009237,\n",
              " -0.054486282,\n",
              " 0.0007228347,\n",
              " 0.009092432,\n",
              " -0.012023051,\n",
              " -0.014550624,\n",
              " -0.03358257,\n",
              " -0.025330383,\n",
              " 0.04694455,\n",
              " 0.0141680725,\n",
              " -0.007241156,\n",
              " -0.0293745,\n",
              " 0.015848568,\n",
              " -0.018335152,\n",
              " -0.0017829639,\n",
              " 0.0034446726,\n",
              " -0.009809717,\n",
              " -0.039867345,\n",
              " -0.004576957,\n",
              " -0.017419761,\n",
              " 0.0062130485,\n",
              " 0.0049970807,\n",
              " -0.010178606,\n",
              " 0.001998149,\n",
              " -0.021559518,\n",
              " -0.0014610399,\n",
              " -0.010014655,\n",
              " 0.008265847,\n",
              " 0.008067741,\n",
              " -0.029975653,\n",
              " 0.016258445,\n",
              " 0.002624919,\n",
              " -0.014673588,\n",
              " 0.025999848,\n",
              " 0.02173713,\n",
              " -0.026764952,\n",
              " -0.022830134,\n",
              " 0.03306339,\n",
              " 0.011551693,\n",
              " -0.022789147,\n",
              " 0.011408236,\n",
              " 0.021942068,\n",
              " 0.003989467,\n",
              " -0.011681488,\n",
              " -0.0030091784,\n",
              " -0.014427662,\n",
              " 0.0015592395,\n",
              " -0.012685685,\n",
              " 0.0035966684,\n",
              " -0.0020425525,\n",
              " 0.0057655997,\n",
              " 0.0047614016,\n",
              " -0.009386177,\n",
              " -0.0022099188,\n",
              " 0.0047716484,\n",
              " 0.022953099,\n",
              " 0.01580758,\n",
              " 0.00784914,\n",
              " 0.04074175,\n",
              " -0.016668322,\n",
              " -0.0016463383,\n",
              " -0.0024695075,\n",
              " 0.0055845706,\n",
              " -0.0005798048,\n",
              " 0.00033942919,\n",
              " 0.018499104,\n",
              " -0.02653269,\n",
              " -0.0058441595,\n",
              " -0.008019921,\n",
              " -0.028827999,\n",
              " -0.013676221,\n",
              " -0.014509637,\n",
              " -0.009406671,\n",
              " 0.00024016216,\n",
              " -0.045687594,\n",
              " 0.01971507,\n",
              " 0.0038630883,\n",
              " -0.01105301,\n",
              " 0.015547992,\n",
              " -0.01804824,\n",
              " 0.0019315442,\n",
              " 0.0020818324,\n",
              " -0.028937299,\n",
              " 0.037817962,\n",
              " 0.006209633,\n",
              " -0.0040543643,\n",
              " -0.0043549403,\n",
              " -0.01388799,\n",
              " 0.024210053,\n",
              " 0.007268481,\n",
              " 0.024442317,\n",
              " 0.02680594,\n",
              " 0.015288402,\n",
              " 0.22384736,\n",
              " -0.024660917,\n",
              " -0.008839675,\n",
              " 0.046561997,\n",
              " 0.011012022,\n",
              " 0.021518528,\n",
              " 0.028718697,\n",
              " 0.028991949,\n",
              " 0.0028298574,\n",
              " 0.010246919,\n",
              " 0.006435065,\n",
              " -0.012105026,\n",
              " -0.018731367,\n",
              " 0.012665192,\n",
              " 0.0045052287,\n",
              " -0.025180096,\n",
              " -0.040140595,\n",
              " -0.027284129,\n",
              " -0.020589475,\n",
              " -0.030768082,\n",
              " 0.010431363,\n",
              " 0.0017266058,\n",
              " 0.015083465,\n",
              " -0.039921995,\n",
              " 0.027653018,\n",
              " 0.007302637,\n",
              " 0.0035317712,\n",
              " 0.01698256,\n",
              " 0.0083000045,\n",
              " 0.00076809194,\n",
              " -0.0030826146,\n",
              " 0.015179102,\n",
              " 0.013525933,\n",
              " 0.0038357633,\n",
              " 0.0049219364,\n",
              " -0.016791284,\n",
              " -0.016832272,\n",
              " -0.0064931307,\n",
              " 0.011271611,\n",
              " 0.023595238,\n",
              " 0.022707172,\n",
              " -0.00737095,\n",
              " 0.011380911,\n",
              " -0.029183224,\n",
              " -0.0027803306,\n",
              " 0.008163379,\n",
              " ...]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Definir una cadena de texto que contiene la consulta de interés.\n",
        "query = '¿Cómo se selecciona un modelo llm?'\n",
        "# Generar la representación vectorial (embedding) de la consulta de texto utilizando la función 'text_embedding'.\n",
        "query_embedding = text_embedding(query)\n",
        "query_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cjrwgq0jJgX"
      },
      "source": [
        "# 3. Búsqueda semántica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bimLQjun3Vsf",
        "outputId": "a4603ad3-f4da-4b5b-f7d2-a54f7a399860"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#########################\n",
            "Contents\n",
            "Introduction to LLMOps 1\n",
            "Why LLMOps? 1\n",
            "Typical stages in an LLMOps Workflow 2\n",
            " 3.1 Data Collection, Preparation, Labelling 2\n",
            " 3.2 Selection of Foundation Models 3\n",
            " 3.3 Using Large Language Models - Prompting and Fine-tuning 4\n",
            " 3.4 Evaluation of Prompts and Models & Version Control 5\n",
            " 3.5 Deployment and Monitoring 6\n",
            " 3.6 Security, Privacy, Governance and Ethical Considerations 8\n",
            "Setting Up LLMOps Pipelines 9\n",
            "Conclusions 11\n",
            "References 11\n",
            "#########################\n",
            "translation and even creative writing.\n",
            "Using large language models in production environments poses a certain unique set of challenges \n",
            "such as organizing LLMs into agents for sub-tasks, developing robust instructions for each  \n",
            "LLM agent, evaluating the correctness of generated response and efficiencies with fine-tuning.  \n",
            "Hence, effective usage in a production environment requires appropriate infrastructure and practices \n",
            "focusing on experimentation, deployment, management and monitoring of large language models. \n",
            "Large Language Model Operations (LLMOps) is a framework of tools and best practices  \n",
            "to manage the lifecycle of LLM-powered applications, from development to deployment  \n",
            "and maintenance.\n",
            "The aim is to enable AI capabilities with LLMs by developing better prompts, longer context, \n",
            "faster inference and customized techniques that enable rapid experimentation and innovation \n",
            "with LLMs. Together, these allow data scientists and engineers to collaborate effectively and\n",
            "#########################\n",
            "|  33.2 Selection of Foundation Models \n",
            "Since training LLMs from scratch is costly and time-consuming, requiring enormous amounts of \n",
            "data and computational resources, many researchers and practitioners opt to use pre-trained LLMs \n",
            "as foundation models, which can be fine-tuned or adapted to specific downstream applications \n",
            "with less data and resources. \n",
            "Some popular foundation models include BERT, GPT-3, T5 and XLNet. These models have been \n",
            "shown to be effective for a wide range of tasks including natural language understanding, natural \n",
            "language generation and machine translation. \n",
            "Different foundation models have different strengths and weaknesses depending on their \n",
            "architecture, pre-training data and learning objectives. It is important to carefully consider the \n",
            "different factors when selecting a foundation model for an LLM project, some points that can be \n",
            "considered are:\n",
            "• Performance of model on benchmark tasks\n",
            "#########################\n",
            "available to run on less expensive hardware. \n",
            "• Availability of domain-specific pre-trained models\n",
            " Domain-specific models are foundational models that are specialized for tasks that can be   \n",
            " selected as per use cases. Some examples are BloombergGPT which is trained on  a wide  \n",
            " range of financial data for the financial industry, Med-PaLM2 and BioMedLM which are   \n",
            " aligned to the medical domain to answer medical questions more accurately and safely.   \n",
            " Galactica which is an LLM fine-tuned with scientific knowledge.\n",
            "• Licensing terms of the foundation model\n",
            " One of the key factors that influence the choice of an LLM is the licensing and availability  \n",
            " of the model. Many organizations can opt for a managed model service provided by various   \n",
            " vendors where for a flat fee they get a fully managed deployment service and technical  \n",
            " support with only the option to fine-tune. Another option is to use an open-source model\n",
            "#########################\n",
            "permitted to perform and may potentially reveal sensitive data .\n",
            "The uncontrolled deployment of LLMs could lead to the generation of biased, offensive or \n",
            "discriminatory content. Proper governance mechanisms are necessary to ensure that the outputs \n",
            "of these models align with ethical standards and do not perpetuate harmful biases. LLMs can \n",
            "inadvertently generate content that promotes hate speech, violence or misinformation.\n",
            "Since these models are trained on large and diverse datasets, inherent biases present in the \n",
            "training data can lead to biased outputs and since many users may not fully understand how the \n",
            "model arrives at specific conclusions, making it difficult to assess the reliability and credibility of the \n",
            "generated content as well.\n",
            "Generating content with LLMs could raise concerns about intellectual property rights. \n",
            "Determining ownership and authorship of generated texts can be challenging, especially when \n",
            "the model incorporates a vast corpus of publicly available data.\n"
          ]
        }
      ],
      "source": [
        "# Definir una función para calcular el producto punto entre dos vectores.\n",
        "def get_dot_product(row):\n",
        "    return np.dot(row, query_vector)\n",
        "\n",
        "# Definir una función para calcular la similitud de coseno entre dos vectores.\n",
        "def cosine_similarity(row):\n",
        "    denominator1 = np.linalg.norm(row)\n",
        "    denominator2 = np.linalg.norm(query_vector.ravel())\n",
        "    dot_prod = np.dot(row, query_vector)\n",
        "    return dot_prod/(denominator1*denominator2)\n",
        "\n",
        "# Definir una función para obtener los fragmentos de texto más relevantes desde un almacenamiento\n",
        "# vectorial dada una consulta.\n",
        "def get_context_from_query(query, vector_store, n_chunks = 5):\n",
        "    global query_vector\n",
        "    # Convertir el embedding de la consulta en un array de numpy.\n",
        "    query_vector = np.array(query_embedding)\n",
        "    # Calcular la similitud de coseno para cada vector en el almacenamiento y ordenar los resultados\n",
        "    # de mayor a menor similitud, seleccionando los índices de los 'n_chunks' más altos.\n",
        "    top_matched = (\n",
        "        vector_store[\"Embedding\"]\n",
        "        .apply(cosine_similarity)\n",
        "        .sort_values(ascending=False)[:n_chunks]\n",
        "        .index)\n",
        "    # Seleccionar los fragmentos de texto correspondientes a los índices de mayor similitud.\n",
        "    top_matched_df = vector_store[vector_store.index.isin(top_matched)][[\"Chunks\"]]\n",
        "    # Devolver una lista con los fragmentos de texto seleccionados.\n",
        "    return list(top_matched_df['Chunks'])\n",
        "\n",
        "# Utilizar la función 'get_context_from_query' para obtener los fragmentos de texto más relevantes\n",
        "# dada la consulta especificada y el almacenamiento vectorial 'df_vector_store', limitando el número\n",
        "# de fragmentos a 5.\n",
        "Context_List = get_context_from_query(\n",
        "    query        = query,\n",
        "    vector_store = df_vector_store,\n",
        "    n_chunks     = 5)\n",
        "\n",
        "# Iterar sobre la lista de fragmentos de texto relevantes e imprimir cada uno, separados por líneas\n",
        "for chunk in Context_List:\n",
        "  print(\"#########################\")\n",
        "  print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Xv0An1qkDnp"
      },
      "source": [
        "# 5. Construir prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UOlfbajEkJUb"
      },
      "outputs": [],
      "source": [
        "custom_prompt = \"\"\"\n",
        "Eres una Inteligencia Artificial super avanzada que trabaja asistente personal.\n",
        "Utilice los RESULTADOS DE BÚSQUEDA SEMANTICA para responder las preguntas del usuario.\n",
        "Solo debes utilizar la informacion de la BUSQUEDA SEMANTICA si es que hace sentido y tiene relacion con la pregunta del usuario.\n",
        "Si la respuesta no se encuentra dentro del contexto de la búsqueda semántica, no inventes una respuesta, y responde amablemente que no tienes información para responder.\n",
        "\n",
        "RESULTADOS DE BÚSQUEDA SEMANTICA:\n",
        "{source}\n",
        "\n",
        "Lee cuidadosamente las instrucciones, respira profundo y escribe una respuesta para el usuario!\n",
        "\"\"\".format(source = str(Context_List))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRFvTqrlk0Aa"
      },
      "source": [
        "# 6. Obtener respuesta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Senk7tcn0b_l",
        "outputId": "2a2e8c29-c09e-4cab-cdae-0cf156ca9bdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La selección de un modelo de Lenguaje de Gran Escala (LLM, por sus siglas en inglés) implica varios factores. Dado que entrenar LLMs desde cero es costoso y consume mucho tiempo, muchos investigadores y profesionales optan por utilizar LLMs preentrenados como modelos base, que pueden ser ajustados o adaptados a aplicaciones específicas con menos datos y recursos.\n",
            "\n",
            "Algunos modelos base populares incluyen BERT, GPT-3, T5 y XLNet. Estos modelos han demostrado ser efectivos para una amplia gama de tareas, incluyendo la comprensión del lenguaje natural, la generación de lenguaje natural y la traducción automática.\n",
            "\n",
            "Al seleccionar un modelo base para un proyecto LLM, es importante considerar varios factores:\n",
            "\n",
            "1. Rendimiento del modelo en tareas de referencia.\n",
            "2. Disponibilidad para ejecutarse en hardware menos costoso.\n",
            "3. Disponibilidad de modelos preentrenados específicos del dominio. Estos son modelos base que están especializados para tareas que pueden ser seleccionadas según los casos de uso. Algunos ejemplos son BloombergGPT, que está entrenado en una amplia gama de datos financieros para la industria financiera, y Med-PaLM2 y BioMedLM, que están alineados con el dominio médico para responder preguntas médicas de manera más precisa y segura.\n",
            "4. Términos de licencia del modelo base. Un factor clave que influye en la elección de un LLM es la licencia y disponibilidad del modelo. Muchas organizaciones pueden optar por un servicio de modelo gestionado proporcionado por varios proveedores, donde por una tarifa plana obtienen un servicio de despliegue totalmente gestionado y soporte técnico con solo la opción de ajustar. Otra opción es utilizar un modelo de código abierto.\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import json\n",
        "# Abrir el archivo 'credentials.json' en modo de lectura.\n",
        "file_name = open('credentials.json')\n",
        "# Cargar el contenido del archivo JSON en una variable llamada 'config_env'.\n",
        "config_env = json.load(file_name)\n",
        "# Extraer el valor asociado a la clave 'openai_key' del diccionario 'config_env'.\n",
        "api_key = config_env[\"openai_key\"]\n",
        "# Crear una instancia de la clase OpenAI, proporcionando la clave API extraída del archivo de configuración.\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# Utilizar el cliente de OpenAI para crear una completación de chat mediante el modelo \"gpt-4\".\n",
        "# Se especifican varios parámetros en la llamada, incluyendo:\n",
        "# - model: el identificador del modelo de lenguaje a utilizar, en este caso \"gpt-4\".\n",
        "# - temperature: controla la aleatoriedad de las respuestas. Un valor de 0.0 genera la respuesta más probable.\n",
        "# - messages: una lista de mensajes que conforman el contexto de la conversación. Se incluyen dos mensajes:\n",
        "#   1. Un mensaje de rol \"system\", que proporciona instrucciones o contexto al modelo.\n",
        "#   2. Un mensaje de rol \"user\", que contiene la consulta del usuario.\n",
        "# 'custom_prompt': Variable definida previamente que contiene el texto del prompt del sistema.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-4\",\n",
        "  temperature = 0.0,\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": custom_prompt},\n",
        "    {\"role\": \"user\", \"content\": query}\n",
        "  ]\n",
        ")\n",
        "\n",
        "# Imprimir el contenido del mensaje de la primera opción de completación generada por el modelo.\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwj-zxtmk-zf"
      },
      "source": [
        "# 7. Ejecutar Streamlit App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9xAs5jSQJRE",
        "outputId": "c5d8b72e-ed81-44c0-e0f2-82ff1a4a9863"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "from pyngrok import ngrok\n",
        "ngrok_token = userdata.get('ngrok_token')\n",
        "!ngrok authtoken $ngrok_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "pi5GqYjBOnL6",
        "outputId": "8281143f-5f76-4e92-8176-ffad58bea448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'http://6f19-34-68-3-131.ngrok-free.app'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!nohup streamlit run app.py &\n",
        "public_url = ngrok.connect(port='8501')\n",
        "public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YTfemLpjQz_Z"
      },
      "outputs": [],
      "source": [
        "# Terminate all active ngrok tunnels\n",
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Naba_SX5n1e-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
